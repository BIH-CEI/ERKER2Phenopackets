{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
   "id": "259770dd648872c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
=======
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Surface\\AppData\\Local\\JetBrains\\Toolbox\\apps\\DataSpell\\ch-0\\232.10072.29\\plugins\\python-ce\\helpers-pro\\jupyter_debug\n",
      "C:\\Users\\Surface\\AppData\\Local\\JetBrains\\Toolbox\\apps\\DataSpell\\ch-0\\232.10072.29\\plugins\\python-ce\\helpers\\pydev\n",
      "C:\\Users\\Surface\\OneDrive\\Documents\\DataSpell\\ERKER2Phenopackets\\ERKER2Phenopackets\\src\\analysis\\ml\n",
      "C:\\Users\\Surface\\OneDrive\\Documents\\DataSpell\\ERKER2Phenopackets\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\python310.zip\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\DLLs\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\lib\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\n",
      "\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\lib\\site-packages\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\lib\\site-packages\\win32\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\lib\\site-packages\\win32\\lib\n",
      "C:\\Users\\Surface\\anaconda3\\envs\\ERKER2Phenopackets\\lib\\site-packages\\Pythonwin\n",
      "..\n",
      "C:\\Users\\Surface\\OneDrive\\Documents\\DataSpell\\ERKER2Phenopackets\\ERKER2Phenopackets\\src\\analysis\\ml\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "import os\n",
    "import sys\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "\n",
    "for path in sys.path:\n",
    "    print(path)\n",
    "sys.path.append('/Users/adam/Documents/BIH/ERKER2Phenopackets/ERKER2Phenopackets')\n",
    "# from ERKER2Phenopackets.src.utils.polars_utils import scatter_plot\n",
    "\n",
<<<<<<< Updated upstream
    "# scatter_plot([1,2,3], [4,4,4,])\n",
    "\n",
    "confidential_phenopackets_path = '/Users/adam/Documents/BIH/ERKER2Phenopackets/ERKER2Phenopackets/ERKER2Phenopackets/data/confidential/phenopackets_mc4r'"
   ]
=======
    "print(os.getcwd())\n",
    "\n",
    "confidential_phenopackets_path = ''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T17:15:40.045368100Z",
     "start_time": "2023-12-15T17:15:39.895660100Z"
    }
   },
   "id": "259770dd648872c"
>>>>>>> Stashed changes
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-15T15:24:17.433211900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from pathlib import Path\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('../../../data/config/config.cfg')\n",
    "\n",
    "phenopackets_out_dir = ('..' / Path(config.get('Paths', 'phenopackets_out'))).resolve()\n",
    "\n",
    "phenopackets_out_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acfd38d139a2a23",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# read in data from phenopackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f871b71d017e1148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.129355100Z",
     "start_time": "2023-12-15T15:24:17.454082900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.io import read_files\n",
    "\n",
    "example_phenopackets_dir = phenopackets_out_dir / 'example-phenopackets-from-synthetic-data'\n",
    "is_synth_data = False\n",
    "pps = read_files(Path(confidential_phenopackets_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34f4c13d1b06892",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# convert phenopackets to dataframe\n",
    "Necessary fields:\n",
    "- id\n",
    "- zygosity\n",
    "- cHGVS mutation\n",
    "- Obsesity Class and timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376850abf12de9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.129355100Z",
     "start_time": "2023-12-15T15:24:17.885924700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Tuple, Any\n",
    "from phenopackets import Phenopacket\n",
    "\n",
    "sex_map = {0: 'unknown', 1: 'female', 2: 'male', 3: 'other'}\n",
    "\n",
    "def extract_phenotype(phenotypic_features, i) -> Tuple[str, str, bool, int]:\n",
    "    phenotypic_features = phenotypic_features[i]\n",
    "    obesity_class_hpo = phenotypic_features.type.id\n",
    "    obesity_class = phenotypic_features.type.label\n",
    "    excluded = phenotypic_features.excluded\n",
    "    onset = phenotypic_features.onset.timestamp.ToSeconds()\n",
    "    return obesity_class_hpo, obesity_class, excluded, onset\n",
    "\n",
    "def extract_fields(phenopacket: Phenopacket):\n",
    "    pp_id = int(phenopacket.id)\n",
    "    dob = phenopacket.subject.date_of_birth.ToSeconds()\n",
    "    sex = sex_map[phenopacket.subject.sex]\n",
    "\n",
    "    try:\n",
    "        variation_descriptor = phenopacket.interpretations[0].diagnosis.genomic_interpretations[0].variant_interpretation.variation_descriptor\n",
    "        c_hgvs = variation_descriptor.expressions[1].value\n",
    "        zygosity = variation_descriptor.allelic_state.label\n",
    "    except IndexError:\n",
    "        return None\n",
    "\n",
    "    phenotypic_features = phenopacket.phenotypic_features\n",
    "\n",
    "    phenotype_data = []\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            obesity_class_hpo, obesity_class, excluded, onset = extract_phenotype(phenotypic_features, i)\n",
    "        except IndexError:\n",
    "            obesity_class_hpo, obesity_class, excluded, onset = None, None, None, None\n",
    "        phenotype_data.extend([obesity_class_hpo, obesity_class, excluded, onset])\n",
    "\n",
    "    return tuple([pp_id, dob, sex, zygosity, c_hgvs] + phenotype_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33a8b9fe2db3454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.129355100Z",
     "start_time": "2023-12-15T15:24:17.897842300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# todo: some people have multiple variants max 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cd4da031dd0c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.129355100Z",
     "start_time": "2023-12-15T15:24:17.915888100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phenopacket = pps[0]\n",
    "variation_descriptor = phenopacket.interpretations[0].diagnosis.genomic_interpretations[0].variant_interpretation\n",
    "\n",
    "phenopacket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c29bfa549db9a2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.130350500Z",
     "start_time": "2023-12-15T15:24:17.929949100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for pp in pps:\n",
    "    print(extract_fields(pp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d37f2348812bb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.131401300Z",
     "start_time": "2023-12-15T15:24:17.947524500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "data = list(filter(lambda x: x is not None, map(extract_fields, pps)))\n",
    "\n",
    "data, len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda56fbf896dc8d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.131401300Z",
     "start_time": "2023-12-15T15:24:17.977438200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['id', 'date_of_birth', 'sex', 'zygosity', 'c_hgvs', 'obesity_class_hpo0', 'obesity_class0', 'phenotype_refuted0', 'onset0', 'obesity_class_hpo1', 'obesity_class1', 'phenotype_refuted1', 'onset1', 'obesity_class_hpo2', 'obesity_class2', 'phenotype_refuted2', 'onset2', 'obesity_class_hpo3', 'obesity_class3', 'phenotype_refuted3', 'onset3', 'obesity_class_hpo4', 'obesity_class4', 'phenotype_refuted4', 'onset4']\n",
    "transposed_data = list(zip(*data))\n",
    "\n",
    "df = pl.DataFrame({col: transposed_data[i] for i, col in enumerate(columns)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd80717f0cb057d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.152340600Z",
     "start_time": "2023-12-15T15:24:17.989488500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cedb3d480161e7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Cleaning\n",
    "\n",
    "There is only one person with an unspecified zygosity, so we can drop this person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ad901aa1294329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.153338500Z",
     "start_time": "2023-12-15T15:24:18.020246200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.filter(df['zygosity'] != 'unspecified zygosity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553c9942c1f450b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.367042600Z",
     "start_time": "2023-12-15T15:24:18.037301900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add new timestamps to make the data more realistic\n",
    "if is_synth_data:\n",
    "    from ERKER2Phenopackets.src.utils.dateutils import date_to_seconds, seconds_to_date, generate_random_date\n",
    "    def random_onsets():\n",
    "        seconds_per_day = 60 * 60 * 24\n",
    "        dob = generate_random_date('1990-01-01', '2010-01-01')\n",
    "        dob_seconds = date_to_seconds(dob)\n",
    "        # dates:\n",
    "        # 1. untersuchung bei geburt\n",
    "        # 2. u6: 10 - 12 monate\n",
    "        # 3. u7: 21 - 24 monate\n",
    "        # 4. u8: 46 - 48 monate\n",
    "        # 5. u9: 60 - 64 monate\n",
    "        intervals = [\n",
    "            (0, 35),\n",
    "            (304, 365),\n",
    "            (639, 730),\n",
    "            (1399, 1460),\n",
    "            (1825, 1947)\n",
    "        ]\n",
    "    \n",
    "        onset_dates = [dob_seconds]\n",
    "        for interval in intervals:\n",
    "            min_days, max_days = interval\n",
    "            min_date = seconds_to_date(dob_seconds + min_days * seconds_per_day)\n",
    "            max_date = seconds_to_date(dob_seconds + max_days * seconds_per_day)\n",
    "            onset_dates.append(date_to_seconds(generate_random_date(min_date, max_date)))\n",
    "    \n",
    "        return onset_dates\n",
    "    \n",
    "    dob, u1, u6, u7, u8, u9 = [], [], [], [], [], []\n",
    "    \n",
    "    for i in range(df.height):\n",
    "        cur_dob, cur_u1, cur_u6, cur_u7, cur_u8, cur_u9 = random_onsets()\n",
    "        dob.append(cur_dob)\n",
    "        u1.append(cur_u1)\n",
    "        u6.append(cur_u6)\n",
    "        u7.append(cur_u7)\n",
    "        u8.append(cur_u8)\n",
    "        u9.append(cur_u9)\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.Series(name='date_of_birth', values=dob),\n",
    "        pl.Series(name='onset0', values=u1),\n",
    "        pl.Series(name='onset1', values=u6),\n",
    "        pl.Series(name='onset2', values=u7),\n",
    "        pl.Series(name='onset3', values=u8),\n",
    "        pl.Series(name='onset4', values=u9),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c487cd2ac76f8303",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:18.473995800Z",
     "start_time": "2023-12-15T15:24:18.327892200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f815cf73e09bf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Preprocessing\n",
    "\n",
    "Order dataframe by descending number of same c_hgvs mutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9535a219122be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.049085900Z",
     "start_time": "2023-12-15T15:24:18.359308200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.polars_utils import sort_by_method\n",
    "\n",
    "def c_hgvs_occurences(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    return  df.join(other=df.groupby('c_hgvs').agg(pl.count('c_hgvs').alias('count')).select(['c_hgvs','count']), on='c_hgvs', how='inner').select(['count'])\n",
    "\n",
    "df = sort_by_method(df, c_hgvs_occurences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27589472e00ef5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.069534700Z",
     "start_time": "2023-12-15T15:24:19.057309900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# correct value of refuted columns to match phenotype: only Obesity has refuted true these are our negatives\n",
    "if is_synth_data:\n",
    "    df = df.with_columns(\n",
    "        phenotype_refuted0=df['obesity_class0'] == 'Obesity',\n",
    "        phenotype_refuted1=df['obesity_class1'] == 'Obesity',\n",
    "        phenotype_refuted2=df['obesity_class2'] == 'Obesity',\n",
    "        phenotype_refuted3=df['obesity_class3'] == 'Obesity',\n",
    "        phenotype_refuted4=df['obesity_class4'] == 'Obesity',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ad56663d877128",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.135875500Z",
     "start_time": "2023-12-15T15:24:19.068536500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.polars_utils import replace_value\n",
    "\n",
    "# growth abnormality: not recorded phenotype -> replace null\n",
    "growth_abnormality = 'Growth abnormality'\n",
    "growth_abnormality_hpo = 'HP:0001507'\n",
    "\n",
    "for i in range(5):\n",
    "    df = replace_value(df, f'obesity_class{i}', growth_abnormality, None)\n",
    "    df = replace_value(df, f'obesity_class_hpo{i}', growth_abnormality_hpo, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90a5108b26b22f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.658919Z",
     "start_time": "2023-12-15T15:24:19.083532400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.analysis.ml.analysis_helper_methods import create_label\n",
    "from ERKER2Phenopackets.src.utils.polars_utils import map_col\n",
    "\n",
    "# 0: Obesity refuted\n",
    "# 1: Overweight\n",
    "# 2: Class I Obesity (HPO)\n",
    "# 3: Class II Obesity (HPO)\n",
    "# 4: Class III Obesity (HPO)\n",
    "\n",
    "for i in range(5):\n",
    "    df = map_col(\n",
    "        df, \n",
    "        map_from=[f'obesity_class_hpo{i}', f'obesity_class{i}', f'phenotype_refuted{i}'], \n",
    "        map_to=f'obesity_label{i}', \n",
    "        mapping=create_label\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f9e817ed5f7428",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.659919700Z",
     "start_time": "2023-12-15T15:24:19.573991800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalize timestamps\n",
    "df = df.with_columns(\n",
    "    onset0=df['onset0'] - df['date_of_birth'],\n",
    "    onset1=df['onset1'] - df['date_of_birth'],\n",
    "    onset2=df['onset2'] - df['date_of_birth'],\n",
    "    onset3=df['onset3'] - df['date_of_birth'],\n",
    "    onset4=df['onset4'] - df['date_of_birth'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7750ed24220a9f44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.663919800Z",
     "start_time": "2023-12-15T15:24:19.588097900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a906342e11eaa2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Overweight, Class I-III Obesity: excluded false\n",
    "if excluded (refuted) true -> BMI in normal range\n",
    "\n",
    "only Obesity has refuted true these are our negatives\n",
    "\n",
    "growth abnormality: not recorded phenotype -> drop rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88fa3f2a117bbf",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Questions\n",
    "## Descriptive Statistics\n",
    "1. Distribution of zygosity? \n",
    "2. Distribution of sex?\n",
    "3. Distribution of c_hgvs mutations?\n",
    "4. c_hgvs distribution by zygosity?\n",
    "5. Obesity Class Distribution?\n",
    "6. Obesity Class Distribution by zygosity?\n",
    "7. Obesity Class Distribution by c_hgvs mutation?\n",
    "8. Obesity Class Distribution c_hgvs mutation and zygosity?\n",
    "9. Obesity Class Distribution by sex?\n",
    "10. How long does it take for a person to develop the different obesity classes based on their mutation?\n",
    "\n",
    "## Predictive Statistics\n",
    "1. What is the probability of a person with a certain c_hgvs mutation to develop a certain obesity class?\n",
    "2. Prediction model of obesity class based on c_hgvs mutation and zygosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33372f6a2112e81f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:19.663919800Z",
     "start_time": "2023-12-15T15:24:19.619714900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from ERKER2Phenopackets.src.utils.polars_utils import barchart, barchart_3d, barchart_subplot, barchart_relative_distribution, barchart_relative_distribution_subplot, piechart_subplot, piechart\n",
    "\n",
    "figsize = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee7ae2399d89428",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Descriptive Statistics\n",
    "# 1. Zygosity Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2ccd79fbfe351b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:20.010730100Z",
     "start_time": "2023-12-15T15:24:19.633755100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_by_zygosity = df.groupby('zygosity').count()\n",
    "num_zygosity = grouped_by_zygosity.height\n",
    "\n",
    "barchart(grouped_by_zygosity['zygosity'], grouped_by_zygosity['count'], figsize=figsize)\n",
    "piechart(grouped_by_zygosity['zygosity'], grouped_by_zygosity['count'], figsize=figsize)\n",
    "\n",
    "grouped_by_zygosity.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8eb453de844943",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 2. Sex Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f481f9fd5acb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:20.278316100Z",
     "start_time": "2023-12-15T15:24:20.005171300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_by_sex = df.groupby('sex').count()\n",
    "\n",
    "barchart(grouped_by_sex['sex'], grouped_by_sex['count'], figsize=figsize)\n",
    "piechart(grouped_by_sex['sex'], grouped_by_sex['count'], figsize=figsize)\n",
    "\n",
    "grouped_by_sex.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcce87fade265ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. c_hgvs variant distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbaec8f1a2ba548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:20.813167700Z",
     "start_time": "2023-12-15T15:24:20.259254100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped_by_c_hgvs = df.groupby('c_hgvs').count().sort('count', descending=True)\n",
    "num_c_hgvs = grouped_by_c_hgvs.height\n",
    "\n",
    "# barchart(grouped_by_c_hgvs['c_hgvs'], grouped_by_c_hgvs['count'], figsize=figsize, x_tick_rotation='vertical')\n",
    "\n",
    "barchart_relative_distribution(grouped_by_c_hgvs['c_hgvs'], grouped_by_c_hgvs['count'], figsize=figsize, x_tick_rotation='vertical')\n",
    "\n",
    "grouped_by_c_hgvs.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f870b6cd2d1fed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:21.405131900Z",
     "start_time": "2023-12-15T15:24:20.808619700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "piechart_subplot(ax1, grouped_by_zygosity['zygosity'], grouped_by_zygosity['count'], 'Zygosity distribution',)\n",
    "barchart_relative_distribution_subplot(ax2, grouped_by_c_hgvs['c_hgvs'], grouped_by_c_hgvs['count'], 'c_hgvs distribution', x_tick_rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d7f6cad7371a2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. c_hgvs distribution by zygosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e565a5ebe61e5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:22.447238400Z",
     "start_time": "2023-12-15T15:24:21.395035600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.polars_utils import contingency_table\n",
    "\n",
    "# barchart_3d(df, 'zygosity', 'c_hgvs', figsize=figsize, grouped_by_col1=grouped_by_zygosity, grouped_by_col2=grouped_by_c_hgvs) # not really useful\n",
    "\n",
    "ct_zygosity_c_hgvs = contingency_table(df, 'zygosity', 'c_hgvs', grouped_by_zygosity['zygosity'], grouped_by_c_hgvs['c_hgvs'])\n",
    "heterozygous_c_hgvs_frquency, homozygous_c_hgvs_frquency = ct_zygosity_c_hgvs[0], ct_zygosity_c_hgvs[1]\n",
    "y_lim = int(np.max(ct_zygosity_c_hgvs) * 1.1) + 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "barchart_subplot(ax1, grouped_by_c_hgvs['c_hgvs'], heterozygous_c_hgvs_frquency, 'Heterozygous c hgvs distribution', x_tick_rotation='vertical', y_min=0, y_max=y_lim)\n",
    "barchart_subplot(ax2, grouped_by_c_hgvs['c_hgvs'], homozygous_c_hgvs_frquency, 'Homozygous c hgvs distribution', x_tick_rotation='vertical', y_min=0, y_max=y_lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505372140f4c91a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:22.960722300Z",
     "start_time": "2023-12-15T15:24:22.440213600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ct_sex_c_hgvs = contingency_table(df, 'sex', 'c_hgvs', ('male', 'female'), grouped_by_c_hgvs['c_hgvs'])\n",
    "male_c_hgvs_frquency, female_c_hgvs_frquency = ct_sex_c_hgvs[0], ct_sex_c_hgvs[1]\n",
    "y_lim = int(np.max(ct_sex_c_hgvs) * 1.1) + 1\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "barchart_subplot(ax1, grouped_by_c_hgvs['c_hgvs'], male_c_hgvs_frquency, 'Male c hgvs distribution', x_tick_rotation='vertical', y_min=0, y_max=y_lim)\n",
    "barchart_subplot(ax2, grouped_by_c_hgvs['c_hgvs'], female_c_hgvs_frquency, 'Female c hgvs distribution', x_tick_rotation='vertical', y_min=0, y_max=y_lim)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c585c1fc6eab3c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Obesity Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b544bde7c11fa5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:23.077351200Z",
     "start_time": "2023-12-15T15:24:22.961735500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.polars_utils import melt_groupby_count, sort_columns\n",
    "\n",
    "# 4.1 count appearance of obesity classes/ hpo terms at each of the 3 inspections\n",
    "obesity_class_count = melt_groupby_count(df, columns=[f'obesity_label{i}' for i in range(5)]).sort(by='value')\n",
    "\n",
    "obesity_class_count = sort_columns(obesity_class_count, ['value'])\n",
    "\n",
    "print(obesity_class_count.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4bc392c533a995",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:23.080322900Z",
     "start_time": "2023-12-15T15:24:22.975842Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "column_names = tuple(obesity_class_count['value'])\n",
    "obesity_class_count_transpose = obesity_class_count.transpose(column_names=column_names)\n",
    "if column_names != obesity_class_count_transpose.row(0):\n",
    "    raise Exception('New column names do not match old row names')\n",
    "obesity_class_count_transpose = obesity_class_count_transpose.tail(5).select(['Refuted Obesity (HP:0001513)', 'Overweight (HP:0025502)', 'Class I obesity (HP:0025499)', 'Class II obesity (HP:0025500)', 'Class III obesity (HP:0025501)'])\n",
    "print(obesity_class_count_transpose.head(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40927090a71ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:23.080322900Z",
     "start_time": "2023-12-15T15:24:22.991290500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cast(column):\n",
    "    return column.cast(pl.Int32).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2fca434653c2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:23.239138200Z",
     "start_time": "2023-12-15T15:24:23.028804200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categories = obesity_class_count_transpose.columns\n",
    "\n",
    "time_stamps = ['U1', 'U6', 'U7', 'U8', 'U9']\n",
    "\n",
    "\n",
    "values = [cast(obesity_class_count_transpose[cat]) for cat in categories]\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "plt.stackplot(range(5), values[0], values[1], values[2], values[3], values[4], labels=categories)\n",
    "plt.xticks(range(5), time_stamps)\n",
    "plt.ylabel('Cases')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaef64706254530",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:23.483925400Z",
     "start_time": "2023-12-15T15:24:23.222201300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.polars_utils import barchart_multiple\n",
    "\n",
    "barchart_multiple(time_stamps, values, 'Cases', categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2593b8bf00d02e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4.1 Phenotype Transition Graphs\n",
    "1. Markov chain style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e950fe645cd8e098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:23.795207800Z",
     "start_time": "2023-12-15T15:24:23.483925400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.analysis.ml.analysis_helper_methods import plot_phenotype_transition_graph\n",
    "\n",
    "file_path = 'markov_chain_obesity_classes.png'\n",
    "phenotype_labels = ['Refuted Obesity \\n(HP:0001513)', 'Overweight \\n(HP:0025502)', 'Class I obesity \\n(HP:0025499)', 'Class II obesity \\n(HP:0025500)', 'Class III obesity \\n(HP:0025501)']\n",
    "\n",
    "initial_counts_df = (melt_groupby_count(df, columns=\n",
    "[f'obesity_label{i}' for i in range(5)])\n",
    "                     .select(['value', 'obesity_label0'])\n",
    "                     )\n",
    "initial_counts_dict = {\n",
    "    row['value']: row['obesity_label0']\n",
    "    for row in initial_counts_df.to_struct('test')\n",
    "}\n",
    "initial_counts = (\n",
    "        [initial_counts_dict[pheno_label]\n",
    "         for pheno_label in categories]\n",
    "        + [initial_counts_dict['Not recorded']]\n",
    ")\n",
    "\n",
    "plot_phenotype_transition_graph(df, 'obesity_label', phenotype_labels.copy(), initial_counts=initial_counts, count_not_recorded=True, percentages=False, file_path=file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ca5b82d5e1c507",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "2. Patient Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682fb2499203bdf7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T15:24:24.453868900Z",
     "start_time": "2023-12-15T15:24:23.802921300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.analysis.ml.analysis_helper_methods import index_similar\n",
    "from ERKER2Phenopackets.src.utils.graphutils import graphplot\n",
    "import networkx as nx\n",
    "\n",
    "# def connect_rows(num_rows, num_nodes_per_row):\n",
    "#     for row in range(1, num_rows):\n",
    "#         for node_in_first_row in range(1, num_nodes_per_row + 1):\n",
    "#             for node_in_next_row in range(1, num_nodes_per_row + 1):\n",
    "#                 current_node_index = (row - 1) * num_nodes_per_row + node_in_first_row\n",
    "#                 next_node_index = row * num_nodes_per_row + node_in_next_row\n",
    "#                 yield current_node_index, next_node_index\n",
    "\n",
    "file_path = 'time_split_graph_obesity_classes.png'\n",
    "num_phenotype_labels = len(phenotype_labels)\n",
    "num_phenotype_measurements = 5\n",
    "source = 0\n",
    "sink = len(categories)*5+1\n",
    "not_recorded = -1\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_node(source, label='Source')\n",
    "G.add_node(sink, label='Sink')\n",
    "G.add_node(not_recorded, label='Not recorded')\n",
    "\n",
    "node = 1\n",
    "empty_nodes = []\n",
    "# transition_counts = {i: [] for i in range(sink)}\n",
    "for i in range(num_phenotype_measurements):\n",
    "    target_addend = 1 + (i + 1) * num_phenotype_labels\n",
    "    \n",
    "    for label in phenotype_labels:\n",
    "        row, col = i, (node - 1) % num_phenotype_labels\n",
    "        count = int(obesity_class_count_transpose.item(row, col))\n",
    "        if count == 0:\n",
    "            empty_nodes.append(node)\n",
    "        else:\n",
    "            G.add_node(node, label=f'{label}\\n{time_stamps[i]}\\n{count}/{df.height}')\n",
    "            \n",
    "            if i < num_phenotype_measurements - 1:\n",
    "                label_wo_newlines = categories[index_similar(categories, label)]\n",
    "                # target nodes and how many patients transition to these targets\n",
    "                outgoing = df.filter(df[f'obesity_label{i}'] == label_wo_newlines).select([f'obesity_label{i + 1}']).groupby(by=[f'obesity_label{i + 1}']).count()\n",
    "                \n",
    "                \n",
    "                for target_label, count_transitioning_to_target in outgoing.rows():\n",
    "                    target = not_recorded\n",
    "                    try:\n",
    "                        target = categories.index(target_label)\n",
    "                    except ValueError:\n",
    "                        pass\n",
    "                    else: # if label in row\n",
    "                        target += target_addend\n",
    "                        \n",
    "                    # add the edge\n",
    "                    if G.has_edge(node, target):\n",
    "                        G.edges[node, target]['denominator'] += count\n",
    "                        G.edges[node, target]['numerator'] += count_transitioning_to_target\n",
    "                    else:\n",
    "                        G.add_edge(node, target, denominator=count, numerator=count_transitioning_to_target)   \n",
    "                \n",
    "        node += 1\n",
    "\n",
    "# connect first row to source\n",
    "for i in range(1, num_phenotype_labels + 1):\n",
    "    if not i in empty_nodes:\n",
    "        G.add_edge(source, i)\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    from_, to, data = edge\n",
    "    if data:\n",
    "        G.edges[from_, to]['label'] = f'{data[\"numerator\"]}/{data[\"denominator\"]}'\n",
    "\n",
    "# connect last row to sink \n",
    "for i in range(len(categories)*4+1, len(categories)*5+1):\n",
    "    if not i in empty_nodes:\n",
    "        G.add_edge(i, sink)\n",
    "G.remove_node(not_recorded)\n",
    "\n",
    "ccs = list(nx.connected_components(G.to_undirected()))\n",
    "# determine largest connected component (this is the main graph) and remove it\n",
    "max_component_id, max_component_length = max(\n",
    "    ((i, len(component)) for i, component in enumerate(ccs)),\n",
    "    key=lambda x: x[1],\n",
    "    default=(-1, 0)\n",
    ")\n",
    "ccs = [component for i, component in enumerate(ccs) if i != max_component_id]\n",
    "\n",
    "for cc in ccs: # assume each cc could have multiple nodes in multiple layers\n",
    "    cc = list(cc)\n",
    "    # separate the sinks and sources of the cc into layers \n",
    "    sources, sinks = [node for node in cc if G.in_degree(node) == 0], [node for node in cc if G.out_degree(node) == 0]\n",
    "    layers = {i: [node for node in cc if i * num_phenotype_labels <= node < (i + 1) * num_phenotype_labels] for i in range(num_phenotype_measurements)}\n",
    "\n",
    "    # 1. \n",
    "    while len(sources) > 0:\n",
    "        for node in sources:\n",
    "            previous = node - num_phenotype_labels\n",
    "            if previous < source:\n",
    "                previous = source\n",
    "\n",
    "            if not G.has_node(previous):\n",
    "                G.add_node(previous, label=f'{phenotype_labels[(previous - 1) % num_phenotype_labels]}\\n{time_stamps[(previous - 1) // num_phenotype_labels]}\\n?/?') # todo\n",
    "\n",
    "            if not G.has_edge(previous, node):\n",
    "                G.add_edge(previous, node, label=\"?\") # todo\n",
    "\n",
    "            if previous != source:\n",
    "                cc.append(previous)\n",
    "\n",
    "        sources = [node for node in cc if G.in_degree(node) == 0]\n",
    "\n",
    "\n",
    "    # 2. find the last layer, go forwards until you hit source and add the connections\n",
    "    while len(sinks) > 0:\n",
    "        # 1. \n",
    "        for node in sinks:\n",
    "            next_ = node + num_phenotype_labels\n",
    "            if next_ > sink:\n",
    "                next_ = sink\n",
    "\n",
    "            if not G.has_node(next_):\n",
    "                G.add_node(next_, label=f'{phenotype_labels[(next_ - 1) % num_phenotype_labels]}\\n{time_stamps[(next_ - 1) // num_phenotype_labels]}\\n?/?') # todo\n",
    "\n",
    "            if not G.has_edge(node, next_):\n",
    "                G.add_edge(node, next_, label=\"?\") # todo\n",
    "\n",
    "            if next_ != sink:\n",
    "                cc.append(next_)\n",
    "\n",
    "        sinks = [node for node in cc if G.out_degree(node) == 0]\n",
    "\n",
    "    # todo: add counts to the edges and nodes that were filled in\n",
    "\n",
    "graphplot(G, file_path=file_path, orientation=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c543b671409dbc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4.2 Table Obesity Class distribution in general and how long it takes to develop each class on average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7d5a9e14a8d564",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T16:50:55.666834300Z",
     "start_time": "2023-12-15T16:50:55.501906Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import polars as pl\n",
    "\n",
    "def time_to_phenotype(df, phenotype_col_prefix, timestamp_col_prefix, time_unit_df: str, time_unit_output: str, by_col_labels: Union[str,List[str]] = None):\n",
    "    \"\"\"\n",
    "    \n",
    "    Expects a polars dataframe in ERKER format, with 5 label columns and 5 timestamp columns. For each patient, it notes when the patient was first diagnosed with a label and then calculates the average time it took for the patient to be diagnosed with the label. This data is then averaged in a new dataframe to show the average time it takes for a patient to be diagnosed with a label. \n",
    "    :param df: \n",
    "    :param phenotype_col_prefix: \n",
    "    :param timestamp_col_prefix:\n",
    "    :param by_col_labels: columns to stratify by\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if by_col_labels and isinstance(by_col_labels, str): # wrap in list\n",
    "        by_col_labels = [by_col_labels]\n",
    "        \n",
    "    label_cols = [col for col in df.columns if col.startswith(phenotype_col_prefix)]\n",
    "    timestamp_cols = [col for col in df.columns if col.startswith(timestamp_col_prefix)]\n",
    "\n",
    "    new_data = {'id': [], 'phenotype': [], 'timestamp': []}\n",
    "    \n",
    "    if by_col_labels:\n",
    "        for by_col_label in by_col_labels:\n",
    "            new_data[by_col_label] = []\n",
    "        for id_, by_cols, labels, timestamps in zip(df['id'], zip(*[df[by_col_label] for by_col_label in by_col_labels]), zip(*[df[col] for col in label_cols]), zip(*[df[col] for col in timestamp_cols])):\n",
    "            seen_labels = set()\n",
    "            for label, timestamp in zip(labels, timestamps):\n",
    "                if label not in seen_labels:\n",
    "                    new_data['id'].append(id_)\n",
    "                    for i, by_col_label in enumerate(by_col_labels):\n",
    "                        new_data[by_col_label].append(by_cols[i])\n",
    "                    new_data['phenotype'].append(label)\n",
    "                    new_data['timestamp'].append(timestamp)\n",
    "                    seen_labels.add(label)\n",
    "        groupby = by_col_labels + ['phenotype']\n",
    "    else:\n",
    "        for id_, labels, timestamps in zip(df['id'], zip(*[df[col] for col in label_cols]),\n",
    "                                           zip(*[df[col] for col in timestamp_cols])):\n",
    "            seen_labels = set()\n",
    "            for label, timestamp in zip(labels, timestamps):\n",
    "                if label not in seen_labels:\n",
    "                    new_data['id'].append(id_)\n",
    "                    new_data['phenotype'].append(label)\n",
    "                    new_data['timestamp'].append(timestamp)\n",
    "                    seen_labels.add(label)\n",
    "        groupby = ['phenotype']\n",
    "\n",
    "    result =  pl.DataFrame(new_data).groupby(groupby).agg(\n",
    "        pl.mean('timestamp').alias('avg_timestamp'),\n",
    "        pl.std('timestamp').alias('std_timestamp'),\n",
    "        pl.count('timestamp').alias('count')\n",
    "    )\n",
    "    \n",
    "    # change time units\n",
    "    unit_switch = {\n",
    "        'seconds': 1,\n",
    "        'minutes': 60,\n",
    "        'hours': 60 * 60,\n",
    "        'days': 60 * 60 * 24,\n",
    "        'months': 60 * 60 * 24 * 30,\n",
    "        'years': 60 * 60 * 24 * 365\n",
    "    }\n",
    "    change_unit = unit_switch[time_unit_df] / unit_switch[time_unit_output]\n",
    "    result = result.with_columns(\n",
    "        (pl.col('avg_timestamp') * change_unit).apply(lambda x: round(x, 2)).alias('avg_timestamp_' + time_unit_output),\n",
    "        (pl.col('std_timestamp') * change_unit).apply(lambda x: round(x, 2)).alias('std_timestamp_' + time_unit_output),\n",
    "    ).drop('avg_timestamp', 'std_timestamp').sort(by='avg_timestamp_' + time_unit_output)\n",
    "    \n",
    "    # add total count and percentage\n",
    "    total_counts = result.groupby(by_col_labels).agg(pl.sum('count').alias('total_count'))\n",
    "    result = result.join(total_counts, on=by_col_labels, how='left').select(\n",
    "        [col for col in result.columns if col != 'count' and col != 'total_count'] + ['count', 'total_count']\n",
    "    )\n",
    "    \n",
    "    result = result.with_columns((pl.col('count') / pl.col('total_count')).alias('frequency'))\n",
    "    \n",
    "    return result\n",
    "output_unit = 'months'\n",
    "avg_time_phenotype = time_to_phenotype(df, 'obesity_label', 'onset', time_unit_df='seconds', time_unit_output=output_unit, by_col_labels=['c_hgvs', 'zygosity'])\n",
    "avg_time_phenotype = avg_time_phenotype.filter(pl.col('phenotype') != 'Not recorded').sort(by='avg_timestamp_' + output_unit)\n",
    "\n",
    "# todo: p values for difference in avg time to phenotype between categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa02e79e8c5baac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T16:50:56.354814600Z",
     "start_time": "2023-12-15T16:50:56.226928200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_time_phenotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a2b103154a4a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T16:51:45.247056700Z",
     "start_time": "2023-12-15T16:51:42.719566300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ERKER2Phenopackets.src.utils.polars_utils import scatter_plot, scatter_subplot\n",
    "# todo: maybe change this to include ranges (std) and not just the mean\n",
    "avg_time_phenotype = avg_time_phenotype.sort(by='phenotype', descending=False)\n",
    "x = avg_time_phenotype['avg_timestamp_months']\n",
    "y_labels = [\"Refuted Obesity (HP:0001513)\", \"Overweight (HP:0025502)\", \"Class I obesity (HP:0025499)\", \"Class II obesity (HP:0025500)\", \"Class III obesity (HP:0025501)\"]\n",
    "phenotyp_to_number = {phenotype: i for i, phenotype in enumerate(y_labels)}\n",
    "y = np.array([phenotyp_to_number[phenotype] for phenotype in avg_time_phenotype['phenotype']])\n",
    "z = avg_time_phenotype['c_hgvs']\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(15,10))\n",
    "ax.axhspan(ymin=3.75, ymax=4.25, xmin=0.05, xmax=0.89, color='red', alpha=0.3, hatch='//')\n",
    "scatter_subplot(ax, x, y, z, title='Average time to phenotype', z_discrete=True, y_labels=y_labels, x_label='Months')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9720170b7730a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T16:32:38.656938700Z",
     "start_time": "2023-12-15T16:32:38.484096300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data = [np.random.normal(0, std, 100) for std in range(1, 4)]\n",
    "plt.violinplot(data, vert=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9d8902644f7672",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Which combinations lead to the development of Class III obesity (HP:0025501) before 5 years of age (60 months)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b303e40388f127",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T16:29:09.121290100Z",
     "start_time": "2023-12-15T16:29:09.042840400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "before_5 = avg_time_phenotype.filter((pl.col('phenotype') == \"Class III obesity (HP:0025501)\") & (pl.col('avg_timestamp_months') < 59))\n",
    "if before_5.is_empty():\n",
    "    print('No combinations lead to the development of Class III obesity (HP:0025501) before 5 years of age (60 months)')\n",
    "else:\n",
    "    before_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643eef176a5ad56",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Which combinations lead to a high risk of developing Class III obesity (HP:0025501)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aad730318dfa4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-15T16:59:50.223281400Z",
     "start_time": "2023-12-15T16:59:50.113629600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "risk_factor = 0.1\n",
    "high_risk = avg_time_phenotype.filter((pl.col('phenotype') == \"Class III obesity (HP:0025501)\") & (pl.col('frequency') > risk_factor))\n",
    "if high_risk.is_empty():\n",
    "    print('No combinations lead to a high risk of developing Class III obesity (HP:0025501)')\n",
    "else:\n",
    "    print(high_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "id": "13814a69e5234085",
=======
   "outputs": [],
   "source": [
    "# todo: std is null for real data for some reason and some patients are counte double in the count column"
   ],
>>>>>>> Stashed changes
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
